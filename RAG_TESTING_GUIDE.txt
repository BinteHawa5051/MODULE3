RAG MODULE - TESTING & TROUBLESHOOTING GUIDE
=============================================

âœ… RAG MODULE IS NOW FIXED AND ENHANCED!

WHAT WAS FIXED:
===============
1. âœ… Better error handling in document parsing
2. âœ… Validation for empty documents
3. âœ… Detailed error messages with tracebacks
4. âœ… Empty chunk filtering
5. âœ… Visual feedback for RAG usage
6. âœ… Test document feature
7. âœ… Smart RAG activation (similarity threshold)
8. âœ… Retrieved context display


HOW TO TEST RAG:
================

METHOD 1: Use Test Document (Easiest)
--------------------------------------
1. Open http://localhost:8501
2. In sidebar, click "ğŸ“ Add Test Document"
3. Wait for success message
4. Try these queries:
   - "What is machine learning?"
   - "Tell me about supervised learning"
   - "What are the applications mentioned in my document?"
   - "Summarize the document"

Expected: Agent uses document context in answers


METHOD 2: Upload Your Own Documents
------------------------------------
1. Prepare documents (PDF, DOCX, or TXT)
2. In sidebar, click "Upload documents for RAG"
3. Select your files
4. Click "Process Documents"
5. Check for success messages showing chunk counts
6. Ask questions about your documents

Supported formats:
- .txt (plain text)
- .pdf (text-based PDFs, not scanned images)
- .docx (Microsoft Word documents)


RAG ACTIVATION TRIGGERS:
=========================

RAG will automatically activate when:

1. High Similarity Score (â‰¥ 0.4)
   - Your query matches document content
   
2. Explicit Document Intent
   - Query contains: "my document"
   - Query contains: "uploaded document"
   - Query contains: "in the document"
   - Query contains: "in my file"
   - Query contains: "according to the document"
   - Query contains: "according to my"

Example queries that trigger RAG:
- "What does my document say about X?"
- "According to the document, what is Y?"
- "Summarize the uploaded document"
- "Find information about Z in my file"


VISUAL INDICATORS:
==================

When RAG is working, you'll see:

1. ğŸ“š Info message: "RAG: Using X document chunks"
2. Expandable section: "ğŸ“– Retrieved Document Context"
   - Shows each chunk with similarity score
   - Shows source filename
   - Shows chunk preview

3. In sidebar: "ğŸ“š RAG Document Status"
   - Lists all ingested files
   - Shows chunk counts per file
   - Shows total chunks


TROUBLESHOOTING:
================

âŒ "No text extracted from document"
â†’ PDF might be scanned (image-based)
â†’ Try converting to text-based PDF or use OCR
â†’ Or use .txt or .docx format

âŒ "No chunks created from document"
â†’ Document might be too short
â†’ Try longer documents (>100 words)

âŒ RAG not activating
â†’ Check if documents are uploaded (see sidebar status)
â†’ Use explicit triggers: "my document", "according to the document"
â†’ Check similarity scores in retrieved context

âŒ "File content is empty"
â†’ File upload failed
â†’ Try re-uploading
â†’ Check file size (very large files may timeout)

âœ… Success indicators:
â†’ Green checkmark with chunk count
â†’ Document appears in RAG status
â†’ Retrieved context shows when querying


TESTING CHECKLIST:
==================

â–¡ 1. Add test document
   Command: Click "ğŸ“ Add Test Document"
   Expected: Success message with chunk count

â–¡ 2. Check RAG status
   Command: Expand "ğŸ“š RAG Document Status"
   Expected: Shows test_document.txt with chunks

â–¡ 3. Test explicit trigger
   Query: "What does my document say about machine learning?"
   Expected: ğŸ“š RAG info appears, uses document context

â–¡ 4. Test implicit trigger (high similarity)
   Query: "What is supervised learning?"
   Expected: ğŸ“š RAG info appears if similarity â‰¥ 0.4

â–¡ 5. View retrieved context
   Command: Expand "ğŸ“– Retrieved Document Context"
   Expected: Shows 3 chunks with scores and sources

â–¡ 6. Upload custom document
   Command: Upload .txt/.pdf/.docx file
   Expected: Success with chunk count

â–¡ 7. Query custom document
   Query: "What does my document say about [topic]?"
   Expected: Uses your document context

â–¡ 8. Clear documents
   Command: Click "ğŸ—‘ï¸ Clear Documents"
   Expected: RAG status shows no documents


EXAMPLE TEST SESSION:
=====================

Step 1: Add test document
â†’ Click "ğŸ“ Add Test Document"
â†’ See: "âœ… Test document added: 4 chunks"

Step 2: Check status
â†’ Expand "ğŸ“š RAG Document Status"
â†’ See: "test_document.txt: 4 chunks"

Step 3: Ask question
â†’ Type: "What is machine learning according to my document?"
â†’ See: "ğŸ“š RAG: Using 3 document chunks"
â†’ See: Retrieved context with scores
â†’ Answer uses document information

Step 4: Test similarity
â†’ Type: "Tell me about reinforcement learning"
â†’ See: RAG activates if similarity high enough
â†’ Answer mentions document content

Step 5: Upload your file
â†’ Upload your own document
â†’ Click "Process Documents"
â†’ See: Success with chunk count

Step 6: Query your file
â†’ Type: "Summarize my document"
â†’ See: RAG uses your document
â†’ Answer based on your content


ADVANCED FEATURES:
==================

1. Multiple Documents
   - Upload multiple files
   - RAG searches across all
   - Shows source in retrieved context

2. Chunk Overlap
   - 50 character overlap between chunks
   - Ensures context continuity

3. Smart Activation
   - Threshold: 0.4 similarity
   - Prevents irrelevant context injection
   - Explicit triggers override threshold

4. Context Display
   - Shows top 3 most relevant chunks
   - Displays similarity scores
   - Shows source filenames
   - Preview of chunk content


RAG PIPELINE ARCHITECTURE:
===========================

1. Document Parser (rag_pipeline.py)
   - Handles PDF, DOCX, TXT
   - Extracts text content
   - Error handling for each format

2. Text Chunker
   - RecursiveCharacterTextSplitter
   - 500 char chunks
   - 50 char overlap

3. Vector Store
   - HuggingFace embeddings
   - sentence-transformers/all-MiniLM-L6-v2
   - Cosine similarity search
   - In-memory storage

4. RAG Pipeline
   - Orchestrates all components
   - Manages document ingestion
   - Retrieves relevant chunks
   - Formats context for LLM


PERFORMANCE NOTES:
==================

- First document: ~5-10 seconds (model download)
- Subsequent documents: ~1-2 seconds
- Query retrieval: <1 second
- Embedding model: Cached after first use
- Vector search: O(n) where n = number of chunks


ALL RAG FEATURES WORKING:
==========================
âœ… Document upload (PDF, DOCX, TXT)
âœ… Text extraction and parsing
âœ… Chunking with overlap
âœ… Vector embeddings (HuggingFace)
âœ… Similarity search
âœ… Context retrieval
âœ… Smart activation (threshold + triggers)
âœ… Visual feedback
âœ… Error handling
âœ… Test document feature
âœ… Multiple document support
âœ… Source tracking
âœ… Context display
